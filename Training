from keras.optimizers import Adam

# Enable mixed precision (for TensorFlow 2.4 and later)


# Compile the model
model.compile(optimizer=Adam(learning_rate=1e-3, beta_2=0.9), loss='mean_squared_error', metrics=['mse'])
from tensorflow.keras.callbacks import ReduceLROnPlateau
# !apt-get install cuda

# # Rest of your imports and code...

# # Set the memory growth option for TensorFlow
# physical_devices = tf.config.list_physical_devices('GPU')
# if physical_devices:
#     try:
#         for device in physical_devices:
#             tf.config.experimental.set_memory_growth(device, True)
#     except:
#         # Invalid device or cannot modify virtual devices once initialized.
#         pass

# Rest of your code...
# Define the learning rate scheduler
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',        # Metric to monitor
    factor=0.1,                # Factor by which the learning rate will be reduced
    patience=3,                # Number of epochs with no improvement before reducing the learning rate
    min_lr=1e-6,               # Lower bound for the learning rate
    verbose=1                  # Prints messages when learning rate is reduced
)

history = model.fit(
    x_train_noisy,   # Original noisy training data
    x_train,         # Original training labels
    batch_size=32,
    epochs=3,
    validation_data=(x_test_noisy, x_test),
    verbose=1,
    callbacks=[lr_scheduler]
)

